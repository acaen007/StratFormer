# Stratformer: Adaptive Multi-Strategy Poker Agent

**Goal:** Build a poker agent that can **detect, model, and exploit suboptimal opponents** while **remaining near-unexploitable** (close to GTO).  
The system learns to reason over a *pool of known strategies*, estimate a **posterior distribution** over likely opponent types, and **select safe counter-strategies** that maximize exploitative EV subject to a GTO deviation budget.

---

## ğŸ” Motivation
Traditional Game-Theory-Optimal (GTO) poker agents are unexploitable but fail to capitalize on weak opponents.  
Conversely, purely exploitative agents win more short-term but can be counter-exploited.  
Stratformer unifies both: an *adaptive* agent that balances **robustness** and **adaptivity** through online opponent modeling.

---

## ğŸ§© Architecture Overview

| Module | Role |
|---------|------|
| `env/` | Wrappers for OpenSpiel games (Kuhn, Leduc, Holdâ€™em). |
| `pool/` | Stores GTO and learned policies, supports sampling and evaluation. |
| `oppmod/` | Opponent modeling via Bayesian, Ridge, and Transformer methods. |
| `novelty/` | Detects when an opponent deviates from all known types. |
| `counter/` | Chooses counter-strategies under KL or exploitability constraints. |
| `online_adapt/` | Lightweight adaptation (LoRA, AutoStep) of neural policies. |
| `eval/` | Computes exploitability, EV, and runs tournaments. |
| `experiments/` | Scripts for Kuhn â†’ Leduc â†’ Holdâ€™em experiments. |

---

## ğŸ§  Core Algorithm (posterior + counter selection)

1. **Track posterior** over pool strategies based on observed actions.  
2. **Select counter** maximizing expected EV â€“ Î» Ã— KL(Ï€â€–Ï€_GTO).  
3. **Detect novelty** if posterior confidence drops below Î±.  
4. **Add new strategy** to the pool and continue learning.  

Mathematically:
\[
\pi^* = \arg\max_{\pi} \mathbb{E}_{\theta \sim P(\theta)} [EV(\pi,\theta)] - \lambda D_{KL}(\pi || \pi_{GTO})
\]

---

## ğŸ§ª Planned Experiments

1. **Kuhn Poker** (tabular policies)  
   - Validate posterior updates & detection.  
   - Compare to pure GTO and naive exploiter.

2. **Leduc Poker** (tabular + small NN policies)  
   - Test scalability and online adaptation.

3. **Texas Holdâ€™em (abstracted)**  
   - Introduce sequence models (Longformer).  
   - Evaluate detection latency and exploitability trade-offs.

---

## ğŸ“Š Evaluation Metrics
- Exploitability (OpenSpiel best-response metric)
- Average EV vs opponent families
- Posterior identification accuracy
- Novelty detection latency / false-positive rate
- Regret and robustness under drifting mixtures

---

## âš™ï¸ Implementation Order
1. **Kuhn Environment & Strategy Pool**
2. **PosteriorTracker (Bayesian)**
3. **KL-Regularized Selector**
4. **Novelty Detector**
5. **Evaluation & Plotting**
6. **Ridge Regression Opponent Model**
7. **LoRA / AutoStep online adaptation**
8. **Leduc integration**
9. **Longformer Opponent Model**
10. **Holdâ€™em abstractions & full experiments**

Each stage will have tests & minimal examples before moving forward.

---

## ğŸ“š References (to be expanded via literature review)
- Brown & Sandholm, *â€œSafe and Nested Subgame Solving for Imperfect-Information Gamesâ€* (AAAI 2017)
- MoravÄÃ­k et al., *â€œDeepStack: Expert-Level AI in No-Limit Pokerâ€* (Science 2017)
- Brown & Sandholm, *â€œSuperhuman AI for multiplayer pokerâ€* (Science 2019)
- Heinrich & Silver, *â€œDeep Reinforcement Learning from Self-Play in Imperfect-Information Gamesâ€* (AAAI 2016)
- Bowling et al., *â€œHeads-Up Limit Holdâ€™em Poker is Solvedâ€* (Science 2015)
- Lanctot et al., *OpenSpiel: A Framework for Reinforcement Learning in Games* (NeurIPS 2019)

---

## ğŸ”¬ Citation
TBD â€” will include paper link once published.

---

## ğŸ§° Environment setup
```bash
conda create -n stratformer python=3.11
conda activate stratformer
pip install -r requirements.txt
